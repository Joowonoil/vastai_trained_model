# 데이터셋 관련 설정
dataset:
  channel_type: ["RMa_Los_50000"] # RMa Transfer
  batch_size: 32 # 학습 및 검증에 사용할 배치 크기
  noise_spectral_density: -174.0  # dBm/Hz # 잡음 스펙트럼 밀도 설정
  subcarrier_spacing: 120.0  # kHz # 서브캐리어 간격 설정
  transmit_power: 30.0  # dBm # 전송 파워 설정
  distance_range: [300.0, 500.0]  # meter # RMa Transfer 전용 거리 범위
  carrier_freq: 28.0  # GHz # 캐리어 주파수 설정
  mod_order: 64 # 변조 방식 차수 설정 (예: 64-QAM)
  ref_conf_dict: # 참조 신호 설정 딕셔너리
    'dmrs': [0, 3072, 6] # DMRS 설정 [시작 인덱스, 끝 인덱스, 스텝] - 원래 모델과 동일하게 복원
  fft_size: 4096 # FFT 크기 설정
  num_guard_subcarriers: 1024 # 가드 서브캐리어 수 설정
  num_symbol: 14 # OFDM 심볼 수 설정
  cp_length: 590  # cyclic prefix length (ns) # Cyclic Prefix (CP) 길이 설정 (나노초)
  max_random_tap_delay_cp_proportion: 0.2  # random tap delay in proportion of CP length # CP 길이 대비 최대 랜덤 탭 지연 비율 설정
  rnd_seed: 0 # 데이터셋 생성 및 셔플링을 위한 랜덤 시드
  num_workers: 0 # 데이터 로딩에 사용할 워커 프로세스 수 (0은 메인 프로세스 사용)
  is_phase_noise: False # 위상 잡음 적용 여부
  is_channel: True # 채널 효과 적용 여부
  is_noise: True # 잡음 적용 여부

# 훈련 관련 설정
training:
  lr: 0.0001 # 학습률 (Learning Rate) 설정
  weight_decay: 0.000001 # 가중치 감소 (Weight Decay) 설정 (L2 정규화)
  max_norm: 1.0 # 그래디언트 클리핑을 위한 최대 L2 노름 값
  num_iter: 60000 # 전이학습용 최적 이터레이션 수
  logging_step: 50 # 전이학습용 빠른 로깅
  evaluation_step: 2000 # 전이학습 평가 간격
  evaluation_batch_size: 4 # 검증 시 사용할 배치 크기
  pn_train_start_iter: 50000 # 위상 잡음 훈련 시작 이터레이션 (현재 위상 잡음 추정 비활성화로 사용되지 않음)
  use_scheduler: False # 학습률 스케줄러 사용 여부
  num_warmup_steps: 0 # warm-up 단계 수 (스케줄러 사용 시 유효)
  # Early Stopping 비활성화 (논문용 안정성 확보)
  use_early_stopping: false
  device: 'cuda:0' # 훈련에 사용할 디바이스 (예: 'cuda:0', 'cpu')
  use_wandb: True # Weights & Biases 로깅 사용 여부
  wandb_proj: 'DNN_channel_estimation_RMa_LoRA_Transfer' # WandB 프로젝트 이름 (RMa LoRA 전이학습)
  pretrained_model_name: 'Large_estimator_v4_base_final' # 로드할 사전 훈련된 모델 이름
  ch_loss_weight: 1 # 채널 손실에 적용할 가중치
  saved_model_name: 'Large_estimator_v4_to_RMa_optimized' # 훈련 완료 후 저장할 모델 이름 (RMa LoRA 전이학습 - 최적화됨)
  num_freeze_layers: 0 # LoRA 방식을 사용하므로 Transformer 레이어 프리징은 0으로 설정 (LoRA 레이어만 학습)
  model_load_mode: 'pretrained' # 모델 로드 방식 선택: 'pretrained' (사전 학습 가중치 로드) 또는 'finetune' (기존 체크포인트 로드)
  load_model_path: '' # 'finetune' 모드일 때 불러올 모델 경로 (예: 'saved_model/your_model_name.pt')
  model_save_step: 5000 # 모델 저장 간격 (이터레이션 수)

# 채널 추정 모델 관련 설정
ch_estimation:
  cond: # ConditionNetwork 설정
    length: 3072 # 입력 데이터 길이
    in_channels: 2 # 입력 채널 수 (실수부, 허수부)
    step_size: 12   # 1224 64 → 128    128 → 64 → 6 → 4 # 스텝 크기
    steps_per_token: 1 # 토큰 당 스텝 수
  transformer: # Transformer 모델 설정
    length: 3072 # 입력 데이터 길이
    channels: 2 # 입력 채널 수
    num_layers: 4        # 기존 Transformer 레이어 수 유지
    d_model: 128         # 모델 차원 유지
    n_token: 256        # 유지
    n_head: 8            # 유지
    dim_feedforward: 1024 # 피드포워드 네트워크 차원 유지
    dropout: 0.1         # 드롭아웃 비율 유지
    activation: 'relu'   # 활성화 함수 유지

  peft: # PEFT (Parameter-Efficient Fine-Tuning) 설정 추가 - 최적화됨
    peft_type: LORA # 사용할 PEFT 타입 (LoRA)
    r: 4 # LoRA 랭크 (rank) - 8에서 4로 축소하여 파라미터 수 절반으로 감소
    lora_alpha: 8 # LoRA 스케일링 팩터 (alpha) - rank에 비례하여 조정
    target_modules: ["mha_q_proj", "mha_v_proj", "ffnn_linear1"] # LoRA를 적용할 모듈 이름 목록 - 핵심 3개만 선택
    lora_dropout: 0.05 # LoRA 드롭아웃 비율

auto_upload:
  enabled: true
  repository: "https://github.com/Joowonoil/vastai_trained_model.git"
  include_config: true
  include_training_log: true