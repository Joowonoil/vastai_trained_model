dataset:
#  channel_type: ["InF_Los_10000", "InF_Nlos_10000", "RMa_Los_10000",]
  channel_type: ["InF_Los", "InF_Nlos", "InH_Los", "InH_Nlos", "RMa_Los", "RMa_Nlos", "UMa_Los", "UMa_Nlos", "UMi_Los", "UMi_Nlos"]
#  phase_noise_type: ["A", "B", "C"]
  batch_size: 32
  noise_spectral_density: -174.0  # dBm/Hz
  subcarrier_spacing: 120.0  # kHz
  transmit_power: 30.0  # dBm
  distance_range: [10.0, 500.0]  # meter
  carrier_freq: 28.0  # GHz
  mod_order: 64
  ref_conf_dict:
    'dmrs': [0, 3072, 6]
#    'ptrs': [6, 3072, 48]
  fft_size: 4096
  num_guard_subcarriers: 1024
  num_symbol: 14
  cp_length: 590  # cyclic prefix length (ns)
  max_random_tap_delay_cp_proportion: 0.2  # random tap delay in proportion of CP length
  rnd_seed: 0
  num_workers: 0
  is_phase_noise: False
  is_channel: True
  is_noise: True

training:
  lr: 0.0001
  weight_decay: 0.000001
  max_norm: 1.0
  num_iter: 200000  # 논문용 충분한 학습
  logging_step: 100
  evaluation_step: 5000
  evaluation_batch_size: 4
  pn_train_start_iter: 50000
  use_scheduler: true  # Cosine annealing 사용
  num_warmup_steps: 1000 # warm-up 단계 수 추가
  device: 'cuda:0' # 훈련에 사용할 디바이스
  use_wandb: True # Weights & Biases 로깅 사용 여부
  wandb_proj: 'DNN_channel_estimation_v4_base' # WandB 프로젝트 이름 (v4 베이스 모델)
  ch_loss_weight: 1 # 채널 손실에 적용할 가중치
  saved_model_name: 'Large_estimator_v4_base' # 훈련 완료 후 저장할 모델 이름
  load_model_path: '' # 계속 훈련할 모델 경로 (필요한 경우에만)
  model_save_step: 10000 # 더 빈번한 저장으로 안전성 확보
  optimizer: 'Adam' # 옵티마이저 타입: 'Adam', 'AdamW', 'SGD'
  momentum: 0.9 # SGD 사용 시 모멘텀 값
  # scheduler: cyclic    # 스케줄러 정보 추가
  # base_lr: 0.0000005    # CyclicLR 최소 학습률 0.000001
  # max_lr: 0.00001      # CyclicLR 최대 학습률  0.00002
  # step_size_up: 4000   # 학습률 증가 주기

# 자동 모델 업로드 설정
auto_upload:
  enabled: true  # 자동 업로드 활성화 여부
  repository: "https://github.com/Joowonoil/vastai_trained_model.git"  # 업로드할 저장소
  include_config: true  # 설정 파일도 함께 업로드
  include_training_log: true  # 훈련 로그 포함

ch_estimation:
  cond:
    length: 3072
    in_channels: 2
    step_size: 12   # 1224 64 → 128    128 → 64 → 6 → 4
    steps_per_token: 1 # 토큰 당 학습 단계 증가 (1 → 2) # 1224 2 → 1  1 → 2 → 3
  transformer:
    length: 3072
    channels: 2
    num_layers: 4        # 레이어 수 증가1 (2 → 4) 레이어 수 증가2 (4 → 6) # 1224 6 → 4  # 250103 4 → 6 → 8
    d_model: 128         # 모델 차원 증가 (128 → 256) # 1224 256 → 128 # 모델 차원 증가 128 → 256
    n_token: 256        # 1224 256 → 24 → 256
    n_head: 8            # 헤드 수 증가 (4 → 8) 헤드 수 증가2 (8 → 16) # 1224 16 → 8 # 헤드 수 증가 (8 → 12 → 16)
    dim_feedforward: 1024 # FFN 차원 증가1 (128 → 256) FFN 차원 증가2 (256 → 512) FFN 차원 증가3 (512 → 1024) # 1224 1024 → 512 FFN 차원 증가3 (512 → 1024 → 2048)
    dropout: 0.1         # 드롭아웃 0.1 → 0.2
    activation: 'relu'   # 활성화 함수 변경 (relu → gelu)

#pn_estimation:
#  cond:
#    length: 896
#    in_channels: 2
#    step_size: 64
#    steps_per_token: 1
#  transformer:
#    length: 14
#    channels: 1
#    num_layers: 2        # 레이어 수 증가 (2 → 3)
#    d_model: 32          # 모델 차원 증가 (32 → 64)
#    n_token: 14
#    n_head: 2            # 유지 (복잡도 적절히 제한)
#    dim_feedforward: 128 # FFN 차원 증가 (32 → 128)
#    dropout: 0.1         # 드롭아웃 추가
#    activation: 'relu'
